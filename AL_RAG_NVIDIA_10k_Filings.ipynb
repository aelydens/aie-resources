{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOc30nsCbQCnUmXEA+s49Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aelydens/aie-resources/blob/main/AL_RAG_NVIDIA_10k_Filings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NVIDIA 10k Filings RAG with Langchain"
      ],
      "metadata": {
        "id": "oCn-Hjt4pHCH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deliverables:\n",
        "\n",
        "Build 🏗️\n",
        "* Data: NVIDIA 10-k Filings\n",
        "* Model: OpenAI text-3-embedding small, GPT-3.5-turbo\n",
        "* Tooling: LangChain or LlamaIndex (you choose)\n",
        "* Vector Store: FAISS\n",
        "* Additional Component: Add one of the following: 1) visibility with WandB OR 2) evaluation with RAGAS\n",
        "Ship 🚢\n",
        "\n",
        "Evaluate your answers to the following questions\n",
        "* \"Who is the E-VP, Operations - and how old are they?\"\n",
        "* \"What is the gross carrying amount of Total Amortizable Intangible Assets for Jan 29, 2023?\"\n",
        "* Record <10 min loom video walkthrough"
      ],
      "metadata": {
        "id": "qp-YMyisozNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain langchain-core langchain-community langchain-openai pymupdf faiss_cpu"
      ],
      "metadata": {
        "id": "syOdjN1DKf9-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3vgbxsoKsHD",
        "outputId": "04358ea2-6d25-45f9-cdb4-2d007f5c38b0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize our embedding model to be `text-embedding-3-small`."
      ],
      "metadata": {
        "id": "mWAZCH6APtgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
      ],
      "metadata": {
        "id": "BLzXyE0FPr-8"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and split our documents."
      ],
      "metadata": {
        "id": "psM1i-x9PlOa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "UwJLCbKcn-aq"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "loader = PyMuPDFLoader(\"https://d18rn0p25nwr6d.cloudfront.net/CIK-0001045810/1cbe8fe7-e08a-46e3-8dcc-b429fc06c1a4.pdf\")\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "chunks = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up our FAISS-powered vector store, and create a retriever."
      ],
      "metadata": {
        "id": "QnspeewdPdjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "vector_store = FAISS.from_documents(documents, embeddings)\n",
        "\n",
        "retriever = vector_store.as_retriever()"
      ],
      "metadata": {
        "id": "hUt1Ksj3OPZO"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create our prompt using `ChatPromptTemplate`."
      ],
      "metadata": {
        "id": "-3vY44adP_s6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context. If you cannot answer the question with the context, please respond with 'I don't know':\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "sOB4J62xOcTg"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize our OpenAI model with a temperature of 0."
      ],
      "metadata": {
        "id": "40O-kJ-aQM3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "openai_chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
      ],
      "metadata": {
        "id": "4eNZsMZYOv1P"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create our LCEL chain."
      ],
      "metadata": {
        "id": "yvp-Lhz-QRrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": prompt | openai_chat_model, \"context\": itemgetter(\"context\")}\n",
        ")\n",
        ""
      ],
      "metadata": {
        "id": "xVI0AqBIOq-3"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question time!"
      ],
      "metadata": {
        "id": "C_MvMVVdPWSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Who is the E-VP, Operations - and how old are they?\"\n",
        "\n",
        "result = retrieval_augmented_qa_chain.invoke({\"question\" : question})\n",
        "\n",
        "print(result[\"response\"].content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LGU0fByO2xW",
        "outputId": "09999992-443f-4045-a152-d231deef94f6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debora Shoquist is the Executive Vice President of Operations, and she is 69 years old.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is the gross carrying amount of Total Amortizable Intangible Assets for Jan 29, 2023 in billions?\"\n",
        "\n",
        "result = retrieval_augmented_qa_chain.invoke({\"question\" : question})\n",
        "\n",
        "print(result[\"response\"].content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xl5uaXYUPDZR",
        "outputId": "8f726398-d63e-477d-ef05-7dffdb8483ae"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$3.539 billion\n"
          ]
        }
      ]
    }
  ]
}