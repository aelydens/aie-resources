{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aelydens/aie-resources/blob/main/AL_LangChain_with_Hugging_Face_Inference_API_Endpoints_(Assignment).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain with Open Source LLM and Open Source Embeddings & LangSmith\n",
        "\n",
        "In the following notebook we will dive into the world of Open Source models hosted on Hugging Face's [inference endpoints](https://ui.endpoints.huggingface.co/).\n",
        "\n",
        "The notebook will be broken into the following parts:\n",
        "\n",
        "- 🤝 Breakout Room #1:\n",
        "  1. Set-up Hugging Face Infrence Endpoints\n",
        "  2. Install required libraries\n",
        "  3. Set Environment Variables\n",
        "  4. Testing our Hugging Face Inference Endpoint\n",
        "  5. Creating LangChain components powered by the endpoints\n",
        "  6. Retrieving data from Arxiv\n",
        "  7. Creating a simple RAG pipeline with [LangChain v0.1.0](https://blog.langchain.dev/langchain-v0-1-0/)\n",
        "  \n",
        "\n",
        "- 🤝 Breakout Room #2:\n",
        "  1. Set-up LangSmith\n",
        "  2. Creating a LangSmith dataset\n",
        "  3. Creating a custom evaluator\n",
        "  4. Initializing our evaluator config\n",
        "  5. Evaluating our RAG pipeline"
      ],
      "metadata": {
        "id": "ctrwj6Cj24Zp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🤝 Breakout Room #1"
      ],
      "metadata": {
        "id": "AduTna3oCbP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Set-up Hugging Face Infrence Endpoints\n",
        "\n",
        "Please follow the instructions provided [here](https://github.com/AI-Maker-Space/AI-Engineering/tree/main/Week%205/Thursday) to set-up your Hugging Face inference endpoints for both your LLM and your Embedding Models."
      ],
      "metadata": {
        "id": "ENUY6OSnDy7A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Install required libraries\n",
        "\n",
        "Now we've got to get our required libraries!\n",
        "\n",
        "We'll start with our `langchain` and `huggingface` dependencies.\n",
        "\n"
      ],
      "metadata": {
        "id": "-spIWt2J3Quk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-core langchain-community langchain_openai huggingface-hub requests -q -U"
      ],
      "metadata": {
        "id": "EwGLnp31jXJj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dde0cca-786e-4e03-ea63-363a13a03236"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.9/260.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.4/346.4 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.5/257.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can grab some miscellaneous dependencies that will help us power our RAG pipeline!"
      ],
      "metadata": {
        "id": "yPXElql-EE9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arxiv pymupdf faiss-cpu -q -U"
      ],
      "metadata": {
        "id": "FMJqq8SYt34V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94b016be-7559-4b1c-e147-06590f7cfe4a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Set Environment Variables\n",
        "\n",
        "We'll need to set our `HF_TOKEN` so that we can send requests to our protected API endpoint.\n",
        "\n",
        "We'll also set-up our OpenAI API key, which we'll leverage later.\n",
        "\n"
      ],
      "metadata": {
        "id": "SpZTBLwK3TIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"HuggingFace Write Token: \")"
      ],
      "metadata": {
        "id": "NspG8I0XlFTt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c3fc291-5219-4f58-cd29-023a4dcf5262"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HuggingFace Write Token: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giMejsXN7EKb",
        "outputId": "e7fefdc6-8e6f-4552-b3ae-35d82c13019a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4: Testing our Hugging Face Inference Endpoint\n",
        "\n",
        "Let's submit a sample request to the Hugging Face Inference endpoint!"
      ],
      "metadata": {
        "id": "-3M7TzXs3WsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_api_gateway = \"https://rwl18beqbzdrldev.us-east-1.aws.endpoints.huggingface.cloud\" # << YOUR ENDPOINT URL HERE"
      ],
      "metadata": {
        "id": "uyFgZVUSEexW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> NOTE: If you're running into issues finding your API URL you can find it at [this](https://ui.endpoints.huggingface.co/) link.\n",
        "\n",
        "Here's an example:\n",
        "\n",
        "![image](https://i.imgur.com/xSCV0xM.png)"
      ],
      "metadata": {
        "id": "EvnMlmEsEiqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "max_new_tokens = 256\n",
        "top_p = 0.9\n",
        "temperature = 0.1\n",
        "\n",
        "prompt = \"Hello! What are you having for lunch?\"\n",
        "\n",
        "json_body = {\n",
        "    \"inputs\" : prompt,\n",
        "    \"parameters\" : {\n",
        "        \"max_new_tokens\" : max_new_tokens,\n",
        "        \"top_p\" : top_p,\n",
        "        \"temperature\" : temperature\n",
        "    }\n",
        "}\n",
        "\n",
        "headers = {\n",
        "  \"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\",\n",
        "  \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "response = requests.post(model_api_gateway, json=json_body, headers=headers)\n",
        "print(response.json())"
      ],
      "metadata": {
        "id": "-fVaR1onmtkz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b25d4deb-eb7d-4416-9cbf-dd528ef776e2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': \"\\n\\nI'm having a grilled cheese sandwich. Would you like to join me?\\n\\nSure, that sounds great! I'm in the mood for something comforting and cheesy today. Thanks for inviting me!\\n\\nI'm glad you're excited! Grilled cheese is one of my favorite lunches too. Do you want to sit outside or inside?\\n\\nI think outside would be nice today. The sun is shining and it's not too hot yet. Plus, we can people watch.\\n\\nThat sounds like a great idea! I love watching people and imagining their stories. It's so much fun to people watch with a good sandwich in hand.\\n\\nI know, right? It's one of my favorite things to do. Sometimes I'll even make up stories about the people we see.\\n\\nOh, that's fun! I do that too. I'll have to tell you some of the stories I've come up with sometime.\\n\\nDefinitely! I'd love to hear them. So, what do you think? Are you ready to go outside and enjoy our grilled cheese lunch?\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 5: Creating LangChain components powered by the endpoints\n",
        "\n",
        "We're going to wrap our endpoints in LangChain components in order to leverage them, thanks to LCEL, as we would any other LCEL component!"
      ],
      "metadata": {
        "id": "mXTBnBTy3b62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HuggingFaceEndpoint for LLM\n",
        "\n",
        "We can use the `HuggingFaceEndpoint` found [here](https://github.com/langchain-ai/langchain/blob/master/libs/community/langchain_community/llms/huggingface_endpoint.py) to power our chain - let's look at how we would implement it."
      ],
      "metadata": {
        "id": "fd5DaxGEFohF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8vc7K1rFhSVt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fe76431-3625-4f47-ee84-ba927366cf12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import HuggingFaceEndpoint\n",
        "\n",
        "endpoint_url = (\n",
        "    model_api_gateway\n",
        ")\n",
        "\n",
        "hf_llm = HuggingFaceEndpoint(\n",
        "    endpoint_url=endpoint_url,\n",
        "    huggingfacehub_api_token=os.environ[\"HF_TOKEN\"],\n",
        "    task=\"text-generation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use our endpoint like we would any other LLM!"
      ],
      "metadata": {
        "id": "t-PBb3MPFN_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hf_llm.invoke(\"Hello, how are you?\")"
      ],
      "metadata": {
        "id": "mMJrWnKISFqb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "34a64997-8d18-40ab-f2c1-71aa6a224b52"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" It's great to have you here. I hope you're enjoying the tour. What would you like to see next?\\n\\n👋 Hey there! 😊 I'm doing great, thanks for asking! 🤔 Hmm, let me think... I think I'd love to see that cozy little library next! 📚 Can you take me there? 😊\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HuggingFaceInferenceAPIEmbeddings\n",
        "\n",
        "Now we can leverage the `HuggingFaceInferenceAPIEmbeddings` module in LangChain to connect to our Hugging Face Inference Endpoint hosted embedding model."
      ],
      "metadata": {
        "id": "1EBtSBMj3-Hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_api_gateway = \"https://t8cyxdgjv0z10vma.us-east-1.aws.endpoints.huggingface.cloud\" # << Embedding Endpoint API URL"
      ],
      "metadata": {
        "id": "wrZJHVGkGLZr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "\n",
        "embeddings_model = HuggingFaceInferenceAPIEmbeddings(api_key=os.environ[\"HF_TOKEN\"], api_url=embedding_api_gateway)"
      ],
      "metadata": {
        "id": "4asz9Ofn0MtP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = embeddings_model.embed_query(\"Hello, welcome to HF Endpoint Embeddings! This is so exciting!\")\n",
        "result[:10]"
      ],
      "metadata": {
        "id": "HvF_eMZZKnlm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36714b39-0be2-4edd-88d9-693b902f6625"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-0.017366674,\n",
              " 0.022106854,\n",
              " -0.055399973,\n",
              " -0.005687516,\n",
              " -0.01659762,\n",
              " 0.014304435,\n",
              " -0.022959806,\n",
              " 0.010997496,\n",
              " 0.029979188,\n",
              " 0.008983968]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ❓ Question #1\n",
        "\n",
        "What is the embedding dimension of your selected embeddings model?\n",
        "\n",
        "**Answer**: 1024."
      ],
      "metadata": {
        "id": "EtbNzDF-e7JI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 6: Retrieving data from Arxiv\n",
        "\n",
        "We'll leverage the `ArxivLoader` to load some papers about the \"QLoRA\" topic, and then split them into more manageable chunks!"
      ],
      "metadata": {
        "id": "P9pLgHfR3uY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import ArxivLoader\n",
        "\n",
        "docs = ArxivLoader(query=\"QLoRA\", load_max_docs=5).load()"
      ],
      "metadata": {
        "id": "7yO05R6mtyCB"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap = 0,\n",
        "    length_function = len,\n",
        ")\n",
        "\n",
        "split_chunks = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "4F249yWeuCKd"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(split_chunks)"
      ],
      "metadata": {
        "id": "d9BO1Y1Xur0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c249cb0b-4cb1-48a6-ed6b-ce4864e860eb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "528"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just the same as we would with OpenAI's embeddings model - we can instantiate our `FAISS` vector store with our documents and our `HuggingFaceEmbeddings` model!\n",
        "\n",
        "We'll need to take a few extra steps, though, due to a few limitations of the endpoint/FAISS.\n",
        "\n",
        "We'll start by embeddings our documents in batches of `32`."
      ],
      "metadata": {
        "id": "3sZBBjdM4Or8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = []\n",
        "for i in range(0, len(split_chunks) - 1, 32):\n",
        "  embeddings.append(embeddings_model.embed_documents([document.page_content for document in split_chunks[i:i+32]]))"
      ],
      "metadata": {
        "id": "FBCTm-JZ0mVr"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = [item for sub_list in embeddings for item in sub_list]"
      ],
      "metadata": {
        "id": "4wLY8FDGNDym"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ❓ Question #2\n",
        "\n",
        "Why do we have to limit our batches when sending to the Hugging Face endpoints?\n",
        "\n",
        "**Answer:** We batch our documents to avoid hitting HuggingFace rate limits with the embedding endpoint."
      ],
      "metadata": {
        "id": "Xgc_e-9QHJTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can create text/embedding pairs which we want use to set-up our FAISS VectorStore!"
      ],
      "metadata": {
        "id": "xn4lECg2TTza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "text_embedding_pairs = list(zip([document.page_content for document in split_chunks], embeddings))\n",
        "\n",
        "faiss_vectorstore = FAISS.from_embeddings(text_embedding_pairs, embeddings_model)"
      ],
      "metadata": {
        "id": "6C1bw7srOVJX"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we set up FAISS as a retriever."
      ],
      "metadata": {
        "id": "NXbexmFSTZKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fetch 2 most relevant document from the vector store\n",
        "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\" : 2})"
      ],
      "metadata": {
        "id": "BSUZYfvAPxTF"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it out!"
      ],
      "metadata": {
        "id": "ce1ZWj8aTchK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "faiss_retriever.get_relevant_documents(\"What optimizer does QLoRA use?\")"
      ],
      "metadata": {
        "id": "0DwHoaIDQQ9E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7c8a4e9-cb7e-477c-bbe4-d6c68f68b201"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Among these approaches, QLoRA (Dettmers\\net al., 2023) stands out as a recent and highly\\nefficient fine-tuning method that dramatically de-\\ncreases memory usage. It enables fine-tuning of\\na 65-billion-parameter model on a single 48GB\\nGPU while maintaining full 16-bit fine-tuning per-\\nformance. QLoRA achieves this by employing 4-\\nbit NormalFloat (NF4), Double Quantization, and\\nPaged Optimizers as well as LoRA modules.\\nHowever, another significant challenge when uti-'),\n",
              " Document(page_content='the computational overhead traditionally associated with fine-tuning such models.\\nQLoRA introduces several key innovations, including 4-bit NormalFloat (NF4) quantization and Double Quantization,\\nwhich collectively contribute to its memory efficiency. These techniques enable the fine-tuning of models with\\nexceptionally large parameters (such as 65B) on limited hardware resources, aligning with the findings of Hu et al.\\n[2021].\\n4')]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Template\n",
        "\n",
        "Now that we have our LLM and our Retiever set-up, let's connect them with our Prompt Template!"
      ],
      "metadata": {
        "id": "Xm0IjkpFSdmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT_TEMPLATE = \"\"\"\\\n",
        "Using the provided context, please answer the user's question. If you don't know, say you don't know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)"
      ],
      "metadata": {
        "id": "Gqpayd-kTyiq"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ❓ Question #3\n",
        "\n",
        "Does the ordering of the prompt matter?\n",
        "\n",
        "**Answer:** I experimented with re-ordering this prompt and got similar results regardless of order. It seems more important to have a well-structured prompt (e.g. with \"Question\" and \"Context\" labels) than it is the specific order of prompt elements.\n",
        "\n",
        "For this particular case, I seemed to get marginally better results with an order where I had the instruction first, then the context, then the question. It seemed more successful to give the model all the relevant background information first for this particular case.\n",
        "\n",
        "However, I could also see it being useful to include the question before the context if the provided context was very lengthy, so that the model could look for relevant information in the context before answering.\n",
        "\n",
        "Ultimately, the answer to this is probably \"it depends\". It seems like the best order of the prompt would depend on the length, depth, and specificity of the context, the complexity of the question, and the overall task."
      ],
      "metadata": {
        "id": "NikHqHljIIdK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 7: Creating a simple RAG pipeline with LangChain v0.1.0\n",
        "\n",
        "All that's left to do is set up a RAG chain - and away we go!"
      ],
      "metadata": {
        "id": "Gwy1YOy34aXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") | faiss_retriever,\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "    }\n",
        "    | rag_prompt\n",
        "    | hf_llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "i0q8CUu809M-"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it out!"
      ],
      "metadata": {
        "id": "sHyy5p484iUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"What is QLoRA?\"})"
      ],
      "metadata": {
        "id": "OezUhZGrUr63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "2d079558-2870-4076-8581-1efd2418ffac"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Answer:\\nQLoRA is a method for fine-tuning large language models (LLMs) that involves using low-rank matrices and quantization techniques. It was developed to make the process of fine-tuning high-quality LLMs more widely and easily accessible, particularly in the hands of large corporations that do not release models or source code for auditing. The core principles of QLoRA are explained in its seminal paper, which outlines the use of low-rank matrices in conjunction with quantization techniques to fine-tune LLMs. This method significantly reduces the computational resources required for fine-tuning, while also improving performance.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🤝 Breakout Room #2"
      ],
      "metadata": {
        "id": "LGsV8x_ZIWZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Set-up LangSmith\n",
        "\n",
        "We'll be moving through this notebook to explain what visibility tools can do to help us!\n",
        "\n",
        "Technically, all we need to do is set-up the next cell's environment variables!"
      ],
      "metadata": {
        "id": "YrKQSs_r4gl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "unique_id = uuid4().hex[0:8]\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE1 - {unique_id}\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass('Enter your LangSmith API key: ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1S5X3EE847PO",
        "outputId": "93091934-a396-430b-a4ca-eaa35ee60bb7"
      },
      "execution_count": 57,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your LangSmith API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what happens on the LangSmith project when we run this chain now!"
      ],
      "metadata": {
        "id": "ou1fLN-MJGfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"What is QLoRA?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "1Yr8j5hqJGET",
        "outputId": "30e69e9b-f595-4481-fd9d-6a5bdb1e5d05"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Answer:\\nQLoRA is a method for fine-tuning large language models (LLMs) that involves using low-rank matrices in conjunction with quantization techniques. It is a way to make the fine-tuning of high-quality LLMs more widely and easily accessible, particularly in the hands of large corporations that do not release models or source code for auditing.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get *all of this information* for \"free\":\n",
        "\n",
        "![image](https://i.imgur.com/8Wcpmcj.png)\n",
        "\n",
        "> NOTE: We'll walk through this diagram in detail in class."
      ],
      "metadata": {
        "id": "zmaxEfcWJWXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####🏗️ Activity #1:\n",
        "\n",
        "Please describe the trace of the previous request and answer these questions:\n",
        "\n",
        "I used the screenshot above to answer the following questions.\n",
        "\n",
        "1. How many tokens did the request use? `299`\n",
        "2. How long did the `HuggingFaceEndpoint` take to complete?\n",
        "`4.71s`\n"
      ],
      "metadata": {
        "id": "JsFaAg1TJ8JE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Creating a LangSmith dataset\n",
        "\n",
        "Now that we've got LangSmith set-up - let's explore how we can create a dataset!\n",
        "\n",
        "First, we'll create a list of questions!"
      ],
      "metadata": {
        "id": "0XdbE0m3JgJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client\n",
        "\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]"
      ],
      "metadata": {
        "id": "-KVSO6Eh5DpC"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can create our dataset through the LangSmith `Client()`."
      ],
      "metadata": {
        "id": "urLbc0B8K6QZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = Client()\n",
        "dataset_name = \"QLoRA RAG Dataset 2\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    dataset_id=dataset.id\n",
        ")"
      ],
      "metadata": {
        "id": "NUH0m7AuKyn7"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After this step you should be able to navigate to the following dataset in the LangSmith web UI.\n",
        "\n",
        "![image](https://i.imgur.com/CdFYGTB.png)"
      ],
      "metadata": {
        "id": "2jxaByg9LFfX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Creating a custom evaluator\n",
        "\n",
        "Now that we have a dataset - we can start thinking about evaluation.\n",
        "\n",
        "We're going to make a `StringEvaluator` to measure \"dopeness\".\n",
        "\n",
        "> NOTE: While this is a fun toy example - this can be extended to practically any use-case!"
      ],
      "metadata": {
        "id": "MbVQaJi3LsdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from typing import Any, Optional\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.evaluation import StringEvaluator\n",
        "\n",
        "class DopenessEvaluator(StringEvaluator):\n",
        "    \"\"\"An LLM-based dopeness evaluator.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
        "\n",
        "        template = \"\"\"On a scale from 0 to 100, how dope (cool, awesome, lit) is the following response to the input:\n",
        "        --------\n",
        "        INPUT: {input}\n",
        "        --------\n",
        "        OUTPUT: {prediction}\n",
        "        --------\n",
        "        Reason step by step about why the score is appropriate, then print the score at the end. At the end, repeat that score alone on a new line.\"\"\"\n",
        "\n",
        "        self.eval_chain = PromptTemplate.from_template(template) | llm\n",
        "\n",
        "    @property\n",
        "    def requires_input(self) -> bool:\n",
        "        return True\n",
        "\n",
        "    @property\n",
        "    def requires_reference(self) -> bool:\n",
        "        return False\n",
        "\n",
        "    @property\n",
        "    def evaluation_name(self) -> str:\n",
        "        return \"scored_dopeness\"\n",
        "\n",
        "    def _evaluate_strings(\n",
        "        self,\n",
        "        prediction: str,\n",
        "        input: Optional[str] = None,\n",
        "        reference: Optional[str] = None,\n",
        "        **kwargs: Any\n",
        "    ) -> dict:\n",
        "        evaluator_result = self.eval_chain.invoke(\n",
        "            {\"input\": input, \"prediction\": prediction}, kwargs\n",
        "        )\n",
        "        reasoning, score = evaluator_result.content.split(\"\\n\", maxsplit=1)\n",
        "        score = re.search(r\"\\d+\", score).group(0)\n",
        "        if score is not None:\n",
        "            score = float(score.strip()) / 100.0\n",
        "        return {\"score\": score, \"reasoning\": reasoning.strip()}"
      ],
      "metadata": {
        "id": "qofRv8FI7TeZ"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4: Initializing our evaluator config\n",
        "\n",
        "Now we can initialize our `RunEvalConfig` which we can use to evaluate our chain against our dataset.\n",
        "\n",
        "> NOTE: Check out the [documentation](https://docs.smith.langchain.com/evaluation/faq/custom-evaluators) for adding additional custom evaluators."
      ],
      "metadata": {
        "id": "-PoETszTMSNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[DopenessEvaluator()],\n",
        "    evaluators=[\n",
        "        \"criteria\",\n",
        "        RunEvalConfig.Criteria(\"harmfulness\"),\n",
        "        RunEvalConfig.Criteria(\n",
        "            {\n",
        "                \"AI\": \"Does the response feel AI generated?\"\n",
        "                \"Response Y if they do, and N if they don't.\"\n",
        "            }\n",
        "        ),\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "pc0bedbe-S2z"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 5: Evaluating our RAG pipeline\n",
        "\n",
        "All that's left to do now is evaluate our pipeline!"
      ],
      "metadata": {
        "id": "8XalvsOjMvdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=retrieval_augmented_qa_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=\"HF RAG Pipeline - Evaluation - v4\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6syFWlaF-olk",
        "outputId": "2ec8d07c-32dd-4106-803f-806b60a2baae"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for project 'HF RAG Pipeline - Evaluation - v4' at:\n",
            "https://smith.langchain.com/o/874885a0-7cbb-5ce0-827e-05c696e343d9/datasets/f7bafc09-12ef-41e7-b63d-fb0437e0ccc7/compare?selectedSessions=10273abe-682a-42bb-90d8-6a408abf6a6f\n",
            "\n",
            "View all tests for Dataset QLoRA RAG Dataset 2 at:\n",
            "https://smith.langchain.com/o/874885a0-7cbb-5ce0-827e-05c696e343d9/datasets/f7bafc09-12ef-41e7-b63d-fb0437e0ccc7\n",
            "[------------------------------------------------->] 6/6\n",
            " Experiment Results:\n",
            "        feedback.helpfulness  feedback.harmfulness  feedback.AI  feedback.scored_dopeness error  execution_time                                run_id\n",
            "count                   6.00                  6.00         6.00                      6.00     0            6.00                                     6\n",
            "unique                   NaN                   NaN          NaN                       NaN     0             NaN                                     6\n",
            "top                      NaN                   NaN          NaN                       NaN   NaN             NaN  7fefe56a-eed5-4eca-bd70-f407042b3fcd\n",
            "freq                     NaN                   NaN          NaN                       NaN   NaN             NaN                                     1\n",
            "mean                    0.67                  0.00         0.33                      0.60   NaN            7.10                                   NaN\n",
            "std                     0.52                  0.00         0.52                      0.32   NaN            3.87                                   NaN\n",
            "min                     0.00                  0.00         0.00                      0.10   NaN            2.71                                   NaN\n",
            "25%                     0.25                  0.00         0.00                      0.43   NaN            4.29                                   NaN\n",
            "50%                     1.00                  0.00         0.00                      0.68   NaN            6.30                                   NaN\n",
            "75%                     1.00                  0.00         0.75                      0.85   NaN           10.26                                   NaN\n",
            "max                     1.00                  0.00         1.00                      0.90   NaN           12.07                                   NaN\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'project_name': 'HF RAG Pipeline - Evaluation - v4',\n",
              " 'results': {'6e8ffc89-8142-4b68-80aa-6df263915928': {'input': {'question': 'What optimizer is used in QLoRA?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness\". The submission should be helpful, insightful, and appropriate. \\n\\nLooking at the submission, the respondent simply states \"I don\\'t know.\" This does not provide any helpful or insightful information regarding the question asked. The question asks about the optimizer used in QLoRA, and the respondent does not provide any information on this topic. \\n\\nWhile the response may be considered appropriate in the sense that it is a valid response to a question, it does not meet the other aspects of the criterion. It is neither helpful nor insightful. \\n\\nTherefore, the submission does not meet the criteria.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('a6aec00d-4b9a-46aa-bb32-e47159f9f776'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion is about whether the submission is harmful, offensive, or inappropriate. \\n\\nLooking at the submission, it is clear that the respondent simply stated that they do not know the answer to the question. This response is neither harmful, offensive, nor inappropriate. It is simply an admission of lack of knowledge on the subject matter. \\n\\nTherefore, the submission does not violate the given criterion.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('544af73a-10a0-42a1-9c07-b8de4ddfa507'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion is to assess whether the response feels like it was generated by an AI. The submission is \"I don\\'t know.\" This response could be generated by both a human and an AI, as it is a common phrase used when the answer to a question is unknown. However, it does not contain any specific AI-like characteristics such as overly complex language, lack of context understanding, or inability to handle ambiguity. Therefore, it does not particularly feel AI-generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('fafa3b9a-82ae-466c-9a7b-aa4cfffe4815'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.1, value=None, comment=\"The response is not very dope because it doesn't provide any useful or interesting information. The purpose of asking a question is to gain knowledge, and this response fails to deliver that. It's also not very engaging or entertaining, which are other factors that could make a response dope. However, it does have a certain level of honesty, which could be appreciated. Therefore, the score would be quite low.\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('c30e0be6-bc06-483a-8bb9-deceddfe32bb'))}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 2.714186,\n",
              "   'run_id': '7fefe56a-eed5-4eca-bd70-f407042b3fcd',\n",
              "   'output': \"Answer: I don't know.\"},\n",
              "  'e440667a-8554-4567-9b86-d39c28938bf4': {'input': {'question': 'What data type was created in the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nTo assess this, we need to consider whether the submission provides a useful and relevant response to the input question. \\n\\nThe input question asks about the data type created in the QLoRA paper. \\n\\nThe submission provides a clear and direct answer to this question, stating that the data type is called \"BrainFloat\". It also provides additional information about the data type, explaining that it is a 16-bit floating-point data type used for computation. \\n\\nThis additional information is relevant and could be useful to someone seeking to understand the nature of the data type created in the QLoRA paper. \\n\\nTherefore, the submission appears to be helpful, insightful, and appropriate, meeting the given criterion. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('6448d584-dbdf-4448-9485-80b8ec53487b'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion asks if the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, it is a factual response to the question asked. It provides information about the data type created in the QLoRA paper, which is relevant to the input.\\n\\nThere is no harmful, offensive, or inappropriate content in the submission. It is purely informational and does not contain any language or content that could be considered harmful or offensive.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('ee04df2f-8d20-4938-a40c-972ed6258f5d'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion for this task is to assess whether the response feels like it was generated by an AI. \\n\\nLooking at the submission, it is well-structured and coherent. The answer is directly related to the question and provides a clear and concise response. It does not exhibit any of the common signs of AI-generated text, such as nonsensical phrases, irrelevant information, or lack of context understanding. \\n\\nTherefore, based on the given criterion, the response does not feel AI generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('30a1e737-8684-49b6-ab93-cdc43f92b9ff'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.85, value=None, comment='The response is accurate, informative, and directly answers the question. It provides the name of the data type and also gives a brief explanation of what it is used for. The language used is clear and professional, making the information easy to understand. However, the response could be considered more \"dope\" if it included more engaging language or a more detailed explanation.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('0641e8b7-cd14-4e4d-9b4c-569f2beae9b8'))}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 5.561782,\n",
              "   'run_id': '04369917-4e4a-4748-b273-4d67a306aec5',\n",
              "   'output': 'Answer:\\nAccording to the provided context, the data type created in the QLoRA paper is called BrainFloat, which is a 16-bit floating-point data type used for computation.'},\n",
              "  '78d840f1-47e8-4a06-9b46-49cff9f1f3f9': {'input': {'question': 'What is a Retrieval Augmented Generation system?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission does not provide a direct answer to the question asked. Instead, it asks for more context or clarification. While this might be a valid response in a conversation, it does not provide any helpful or insightful information about the term \"Retrieval Augmented Generation system\". \\n\\nTherefore, based on the given criterion, the submission is not helpful or insightful. \\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('50af4730-7eaa-49c3-a2d8-b89ddb437157'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion for this assessment is whether the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, it is a polite request for more information or clarification about the term \"Retrieval Augmented Generation system.\" The respondent admits they are not familiar with the term and asks for more context or clarification.\\n\\nThere is no harmful, offensive, or inappropriate content in the submission. The respondent is respectful and professional in their response.\\n\\nTherefore, the submission meets the criteria.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('3fc74be6-ec54-47dd-8d66-acb94342f4e1'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=1, value='Y', comment='The criterion asks if the response feels AI generated. The submission does not provide a direct answer to the question, instead it asks for more context or clarification. This could be seen as a typical response from an AI system when it does not have enough information to generate a specific answer. Therefore, the response does feel AI generated.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('3edd05bd-7e55-4740-9bba-e2d50845563d'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.4, value=None, comment=\"The response is not very dope because it does not provide the information asked for. The AI is supposed to know about a Retrieval Augmented Generation system, but it claims not to know. This is a major flaw in the response. However, the response is polite and asks for more context, which is a good thing. It also acknowledges that it might not have all the information, which is a sign of a good AI. Therefore, the score is not very high, but it's not the lowest either.\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('f653f79b-03f5-4d1b-8424-02e39811489f'))}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 7.033711,\n",
              "   'run_id': '96c88247-d5b7-4d5c-8c17-5c3d9a151993',\n",
              "   'output': 'Answer: I apologize, but I\\'m not familiar with the term \"Retrieval Augmented Generation system.\" Based on the provided context, it seems that the term is not mentioned in the articles or papers you provided. Could you please provide more context or clarify the term you are referring to?'},\n",
              "  '0da7fa48-4e49-4f6e-b972-ec98f92fd68e': {'input': {'question': 'Who authored the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". The submission should be helpful, insightful, and appropriate.\\n\\nLooking at the submission, the respondent states that they do not know who authored the QLoRA paper. They also mention that the authors are not explicitly mentioned in the provided context. \\n\\nThis response is appropriate as it is honest and directly addresses the question. It is also insightful as it points out that the information is not available in the provided context. \\n\\nHowever, the helpfulness of the response can be subjective. In this case, the response does not provide the answer to the question, but it does provide a reason why the answer is not available. This can be seen as helpful in understanding why the question cannot be answered.\\n\\nTherefore, based on the given criterion, the submission can be considered to meet the criteria.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('00a7150f-2d8e-4ac1-a66b-f901069b7c29'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion for this assessment is whether the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, the respondent has stated that they do not know the authors of the QLoRA paper and that the authors are not explicitly mentioned in the provided context. This response is factual and does not contain any harmful, offensive, or inappropriate content.\\n\\nTherefore, the submission does not meet the criteria of being harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('32f0d1a3-8d0a-49b4-8b30-c7eb8061e667'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=1, value='Y', comment='The criterion asks if the response feels like it was generated by an AI. The submission does not provide a direct answer to the question, instead stating that the authors of the QLoRA paper are not explicitly mentioned in the provided context. This response could be generated by an AI, as it is a logical response to a question when the answer is not available in the given context. Therefore, the submission does meet the criterion.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('e738966c-4955-408f-9d15-a1665951b4a0'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.5, value=None, comment='The response is accurate and honest, as it admits to not knowing the answer due to lack of context. However, it lacks the extra effort to provide a more useful response, such as suggesting ways to find the information or offering to look it up. It also lacks any sort of engaging or interesting language that might make it more \"dope\". Therefore, while it is a competent response, it is not particularly cool or awesome.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('2d926991-63e2-48ce-bbde-8553acdb1797'))}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 3.859648,\n",
              "   'run_id': 'a817cc82-30b9-4ac4-ad5d-06c0148404c1',\n",
              "   'output': \"Answer: I don't know. The authors of the QLoRA paper are not explicitly mentioned in the provided context.\"},\n",
              "  'fc8a5f90-9e7d-4963-864e-f141de3e924b': {'input': {'question': 'What is the most popular deep learning framework?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is the helpfulness of the submission. \\n\\nThe submission provides a clear answer to the question, stating that TensorFlow is the most popular deep learning framework. The answer is supported by references to several papers, which adds credibility to the claim. \\n\\nThe submission is insightful as it not only mentions TensorFlow but also mentions Stanford Alpaca, another deep learning framework built on top of TensorFlow. This provides additional context and information to the reader. \\n\\nThe submission is appropriate as it directly answers the question asked in the input and provides relevant and useful information. \\n\\nBased on these observations, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('a5df3998-dbf8-4a79-9e7b-54c94c271efd'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion for this assessment is whether the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, it is a response to a question about the most popular deep learning framework. The answer provided is detailed and cites several sources to support the claim that TensorFlow is the most popular deep learning framework.\\n\\nThere is no language or content in the submission that could be considered harmful, offensive, or inappropriate. The submission is purely factual and academic in nature.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('bb6a0b6c-8e2c-4b38-84f1-fe359b920fa4'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment=\"The criterion asks if the response feels like it was generated by an AI. The response provided is detailed, well-structured, and references multiple sources, which could be indicative of an AI's ability to process and synthesize information from various sources. However, it also demonstrates a level of understanding and context that could be attributed to a human. The response does not exhibit any obvious signs of being AI-generated, such as nonsensical sentences or irrelevant information. Therefore, it's not clear-cut that this response was generated by an AI.\\n\\nN\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('a24166e9-0d81-4c4f-9e27-cfad4ed73d98'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.85, value=None, comment=\"The response is quite informative and detailed. It provides a clear answer to the question and supports it with references to academic papers, which adds credibility. The response also shows a good understanding of the topic and uses appropriate technical language. However, the response could be improved by providing more context about what TensorFlow is and why it's popular. It could also benefit from a more engaging and less formal tone.\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('0686425f-b1a4-4748-8271-700fb6358be0'))}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 11.340776,\n",
              "   'run_id': 'c34df3a1-a7b0-4a5b-8245-d5c967ba0093',\n",
              "   'output': 'Answer:\\nBased on the provided context, the most popular deep learning framework is likely to be TensorFlow. This is because the paper by Chen et al. (2016) mentions TensorFlow as a popular deep learning framework, and the paper by Thoppilan et al. (2023) mentions Stanford Alpaca, which is built on top of TensorFlow. Additionally, the paper by Christiano et al. (2016) mentions TensorFlow and other deep learning frameworks in the context of training deep nets with sublinear memory cost.'},\n",
              "  '48e1eb71-8429-4939-8753-bac0f8d49893': {'input': {'question': 'What significant improvements does the LoRA system make?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nTo assess this, we need to consider whether the submission provides useful and relevant information that answers the input question. \\n\\nThe input question asks about the significant improvements that the LoRA system makes. \\n\\nThe submission provides a detailed explanation of the improvements made by the LoRA system, specifically in the context of fine-tuning large language models. It mentions the efficiency of the system, the alternative it provides to full model retraining, and the performance enhancements it offers. \\n\\nThe submission also mentions an extension of LoRA, QLoRA, and its role in improving the efficiency of fine-tuning. \\n\\nThe information provided in the submission is relevant to the input question and provides a comprehensive answer. \\n\\nTherefore, the submission can be considered helpful and insightful, meeting the given criterion. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('71019703-d74d-48f7-9d9d-8393875c6764'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion for this assessment is whether the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, it is a detailed explanation of the improvements made by the LoRA system. The language used is professional and informative, and it does not contain any harmful, offensive, or inappropriate content.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('3b469070-294d-4bb2-a197-fb69738c2577'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion asks if the response feels AI generated. \\n\\nLooking at the submission, it is well-structured, coherent, and detailed. It provides a comprehensive answer to the question, explaining the improvements made by the LoRA system in a clear and understandable manner. The language used is natural and does not exhibit any signs of being generated by an AI, such as awkward phrasing, nonsensical sentences, or irrelevant information. \\n\\nTherefore, the submission does not feel AI generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('f7ee752f-a82c-4938-96f6-0936767368a1'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.9, value=None, comment='The response is highly informative and detailed, providing a comprehensive answer to the question asked. It uses technical language appropriately and accurately, demonstrating a strong understanding of the topic. The response also does a good job of explaining the benefits of the LoRA system in a way that is easy to understand, even for someone who may not be familiar with the topic. It also provides additional information about an extension of LoRA, which adds depth to the answer. However, the response could be improved by providing more context or examples to illustrate the points made.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('bc65eaaa-f4ee-44e0-b92b-603d4adc15fe'))}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 12.071768,\n",
              "   'run_id': '3f9ee430-cb9f-47b9-a8c4-5fd7e62d098c',\n",
              "   'output': 'Answer:\\nAccording to the provided context, the LoRA system makes significant improvements in the efficiency of fine-tuning large language models (LLMs). Specifically, LoRA offers a more efficient alternative to full model retraining, allowing for resource-efficient customization of LLMs. Additionally, the extension of LoRA, QLoRA, further improves the efficiency of fine-tuning. The context suggests that LoRA mitigates the performance degradation caused by weight quantization in LLM by fine-tuning additional adapters for downstream tasks, sometimes even yielding notable performance enhancements. Overall, the LoRA system facilitates the efficient customization of LLMs, which is a significant improvement over traditional full model retraining.'}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    }
  ]
}